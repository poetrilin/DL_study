{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "图 1: SGD(Stochastic Gradient Descent) vs GD\n",
    "“+”表示损失的最小值。 SGD造成许多振荡以达到收敛。但是每个step中，计算SGD比使用GD更快，因为它仅使用一个训练示例（相对于GD的整个批次）。\n",
    "\n",
    "注意：实现SGD总共需要3个for循环：\n",
    "1. 迭代次数\n",
    "2. m个训练数据\n",
    "\n",
    "3. 各层上(要更新所有参数，从(W1,b1)到(Wl,bl))\n",
    "\n",
    "实际上，如果你既不使用整个训练集也不使用一个训练示例来执行每次更新，则通常会得到更快的结果。小批量梯度下降法在每个步骤中使用中间数量的示例。通过小批量梯度下降，你可以遍历小批量，而不是遍历各个训练示例。\n",
    "\n",
    "图 2：SGD vs Mini-Batch GD\n",
    "\n",
    "“+”表示损失的最小值。在优化算法中使用mini-batch批处理通常可以加快优化速度。\n",
    "\n",
    "你应该记住：\n",
    "\n",
    "梯度下降，小批量梯度下降和随机梯度下降之间的差异是用于执行一个更新步骤的数据数量。\n",
    "必须调整超参数学习率。\n",
    "\n",
    "在小批量的情况下，通常它会胜过梯度下降或随机梯度下降（尤其是训练集较大时）。\n",
    "\n",
    "### Mini-Batch 梯度下降+ SGD\n",
    "\n",
    "让我们学习如何从训练集（X，Y）中构建小批次数据。\n",
    "\n",
    "分两个步骤：\n",
    "\n",
    "1. Shuffle :如下所示，创建训练集（X，Y）的随机打乱版本。X和Y中的每一列代表一个训练示例。\n",
    "   \n",
    "2. Partition :将打乱后的（X，Y）划分为大小为mini_batch_size（此处为64）的小批处理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n",
    "    \n",
    "    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n",
    "    m = X.shape[1]                  # number of training examples\n",
    "    mini_batches = []\n",
    "        \n",
    "    # Step 1: Shuffle (X, Y)\n",
    "    permutation = list(np.random.permutation(m))\n",
    "    shuffled_X = X[:, permutation]\n",
    "    shuffled_Y = Y[:, permutation].reshape((1,m))\n",
    "\n",
    "    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n",
    "    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n",
    "    for k in range(0, num_complete_minibatches):\n",
    "        ### START CODE HERE ### (approx. 2 lines)\n",
    "        mini_batch_X = shuffled_X[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch_Y = shuffled_Y[:, k * mini_batch_size : (k+1) * mini_batch_size]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    # Handling the end case (last mini-batch < mini_batch_size)\n",
    "    if m % mini_batch_size != 0:\n",
    "        mini_batch_X = shuffled_X[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch_Y = shuffled_Y[:, num_complete_minibatches * mini_batch_size : m]\n",
    "        mini_batch = (mini_batch_X, mini_batch_Y)\n",
    "        mini_batches.append(mini_batch)\n",
    "\n",
    "    return mini_batches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Momentum\n",
    "\n",
    "因为小批量梯度下降仅在看到示例的子集后才进行参数更新，所以更新的方向具有一定的差异，因此小批量梯度下降所采取的路径将“朝着收敛”振荡。利用冲量则可以减少这些振荡。\n",
    "\n",
    "冲量考虑了过去的梯度以平滑更新。我们将先前梯度的“方向”存储在变量v中。\n",
    "这将是先前步骤中梯度的指数加权平均值，你也可以将v看作是下坡滚动的球的“速度”，根据山坡的坡度/坡度的方向来提高速度（和冲量）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_velocity(parameters):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    \n",
    "    # Initialize velocity\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape)\n",
    "        v[\"db\" + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape)\n",
    "        \n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_momentum(parameters, grads, v, beta, learning_rate):\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    \n",
    "    # Momentum update for each parameter\n",
    "    for l in range(L):\n",
    "        \n",
    "        # compute velocities\n",
    "        v[\"dW\" + str(l + 1)] = beta*v[\"dW\" + str(l + 1)]+(1-beta)*grads['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l + 1)] = beta*v[\"db\" + str(l + 1)]+(1-beta)*grads['db' + str(l+1)]\n",
    "\n",
    "        parameters[\"W\" + str(l + 1)] = parameters['W' + str(l+1)] - learning_rate*v[\"dW\" + str(l + 1)] \n",
    "        parameters[\"b\" + str(l + 1)] = parameters['b' + str(l+1)] - learning_rate*v[\"db\" + str(l + 1)] \n",
    "        \n",
    "    return parameters, v"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意：\n",
    "\n",
    "- 速度用零初始化。因此，该算法将花费一些迭代来“提高”速度并开始采取更大的步骤。\n",
    "- 如果$\\beta =0$，则它变为没有冲量的标准梯度下降。\n",
    "\n",
    "怎样选择$beta$?\n",
    "\n",
    "- 冲量$\\beta$越大，更新越平滑，因为我们对过去的梯度的考虑也更多。但是，如果太大，也可能使更新变得过于平滑。\n",
    "\n",
    "- $\\beta$的常用值范围是0.8到0.999。如果你不想调整它，则0.9通常是一个合理的默认值。\n",
    "\n",
    "-调整模型的最佳$\\beta$可能需要尝试几个值，以了解在降低损失函数$J$的值方面最有效的方法。\n",
    "\n",
    "\n",
    "你应该记住：\n",
    "\n",
    "冲量将过去的梯度考虑在内，以平滑梯度下降的步骤。它可以应用于批量梯度下降，小批次梯度下降或随机梯度下降。\n",
    "必须调整冲量超参数$\\beta$和学习率$\\alpha$。\n",
    "\n",
    "### 4 Adam\n",
    "\n",
    "- t计算出Adam采取的步骤数\n",
    "- L是层数\n",
    "- beta1和beta2是控制两个指数加权平均值的超参数。\n",
    "- alpha是学习率\n",
    "- epsilon是一个很小的数字，以避免被零除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_adam(parameters) :\n",
    "    L = len(parameters) // 2 # number of layers in the neural networks\n",
    "    v = {}\n",
    "    s = {}\n",
    "    \n",
    "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
    "    for l in range(L):\n",
    "        v[\"dW\" + str(l + 1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        v[\"db\" + str(l + 1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "        s[\"dW\" + str(l + 1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
    "        s[\"db\" + str(l + 1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
    "    \n",
    "    return v, s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
    "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
    "    \n",
    "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
    "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
    "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
    "    \n",
    "    # Perform Adam update on all parameters\n",
    "    for l in range(L):\n",
    "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
    "        v[\"dW\" + str(l + 1)] = beta1*v[\"dW\" + str(l + 1)] +(1-beta1)*grads['dW' + str(l+1)]\n",
    "        v[\"db\" + str(l + 1)] = beta1*v[\"db\" + str(l + 1)] +(1-beta1)*grads['db' + str(l+1)]\n",
    "\n",
    "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
    "        v_corrected[\"dW\" + str(l + 1)] = v[\"dW\" + str(l + 1)]/(1-(beta1)**t)\n",
    "        v_corrected[\"db\" + str(l + 1)] = v[\"db\" + str(l + 1)]/(1-(beta1)**t)\n",
    "\n",
    "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
    "        s[\"dW\" + str(l + 1)] =beta2*s[\"dW\" + str(l + 1)] + (1-beta2)*(grads['dW' + str(l+1)]**2)\n",
    "        s[\"db\" + str(l + 1)] = beta2*s[\"db\" + str(l + 1)] + (1-beta2)*(grads['db' + str(l+1)]**2)\n",
    "\n",
    "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
    "        s_corrected[\"dW\" + str(l + 1)] =s[\"dW\" + str(l + 1)]/(1-(beta2)**t)\n",
    "        s_corrected[\"db\" + str(l + 1)] = s[\"db\" + str(l + 1)]/(1-(beta2)**t)\n",
    "\n",
    "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
    "        parameters[\"W\" + str(l + 1)] = parameters[\"W\" + str(l + 1)]-learning_rate*(v_corrected[\"dW\" + str(l + 1)]/np.sqrt( s_corrected[\"dW\" + str(l + 1)]+epsilon))\n",
    "        parameters[\"b\" + str(l + 1)] = parameters[\"b\" + str(l + 1)]-learning_rate*(v_corrected[\"db\" + str(l + 1)]/np.sqrt( s_corrected[\"db\" + str(l + 1)]+epsilon))\n",
    "        \n",
    "    return parameters, v, s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|优化方法|\t准确度|\t模型损失|\n",
    "|---|---|---|\n",
    "|Gradient descent|79.70％\t|振荡|\n",
    "|Momentum|\t79.70％\t|振荡|\n",
    "|Adam\t|94％|\t更光滑|\n",
    "\n",
    "冲量通常会有所帮助，但是鉴于学习率低和数据集过于简单，其影响几乎可以忽略不计。同样，你看到损失的巨大波动是因为对于优化算法，某些小批处理比其他小批处理更为困难。\n",
    "\n",
    "另一方面，Adam明显胜过小批次梯度下降和冲量。如果你在此简单数据集上运行更多epoch，则这三种方法都将产生非常好的结果。但是，Adam收敛得更快。\n",
    "\n",
    "Adam的优势包括：\n",
    "\n",
    "1. 相对较低的内存要求（尽管高于梯度下降和带冲量的梯度下降）\n",
    "2. 即使很少调整超参数，通常也能很好地工作（$\\alpha$除外）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c982fbd28920764c91e446e87fd33654e833f053c95f54547b2708334b603c10"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
